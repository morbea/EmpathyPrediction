{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACtJwPozuX0J",
        "outputId": "d946e61d-71d6-43bc-f3e5-cd65a21d5dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "df = pd.read_csv('/content/drive/MyDrive/עותק של Supplementary data - responses and measures (1).csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOqLjpP4uX0N"
      },
      "source": [
        "Remove punctuation, numbers, and special characters.\n",
        "Convert text to lowercase.\n",
        "Tokenize the paragraphs into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHgwPUaAu8_j",
        "outputId": "80e7f554-f675-4237-f353-8f987dce543c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1884332789.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  all['Condition'] = all['Condition'].apply(lambda x: 2 if x in {'Human', '2'} else 1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "all = (df[df['Condition'].isin({'Human', '2','1','AI'})])\n",
        "# set df['Condition'].isin({'Human', '2'} to 2 and df['Condition'].isin({'AI', '1'} to 1\n",
        "all['Condition'] = all['Condition'].apply(lambda x: 2 if x in {'Human', '2'} else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5KG8pnydjTA"
      },
      "outputs": [],
      "source": [
        "\n",
        "hrel = all[['Response', 'EmpathyQ_1','Condition']]\n",
        "\n",
        "# Rename columns to match script expectations\n",
        "hrel = hrel.rename(columns={\"Response\": \"response\", \"EmpathyQ_1\": \"label\",'Condition':'condition'})\n",
        "\n",
        "hrel = hrel.dropna(subset=[\"response\", \"label\"])  # Drop rows with NaN\n",
        "hrel = hrel[hrel[\"response\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]  # Keep valid strings\n",
        "hrel[\"label\"] = hrel[\"label\"].astype(float)  # Ensure empathy is float\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile ex1.py\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune models for binary empathy classification with weighted loss\")\n",
        "    parser.add_argument(\"--max_train_samples\", type=int, default=-1, help=\"Number of training samples or -1 for all\")\n",
        "    parser.add_argument(\"--max_predict_samples\", type=int, default=-1, help=\"Number of test samples or -1 for all\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for training and evaluation\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Run training\")\n",
        "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Run prediction\")\n",
        "    parser.add_argument(\"--model_path\", type=str, default=\"./results\", help=\"Path to save/load model\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default=\"bert-base-uncased\", help=\"Model name (bert-base-uncased, roberta-base, distilbert-base-uncased, electra-small-discriminator)\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def compute_sample_weights(labels):\n",
        "    \"\"\"Compute sample weights for binary classification based on class frequency\"\"\"\n",
        "    bins = np.array([0, 1, 2])  # Bins for classes 0 and 1\n",
        "    freq, _ = np.histogram(labels, bins=bins)\n",
        "    freq = freq + 1e-6  # Avoid division by zero\n",
        "    weights = np.max(freq) / freq * 2.0\n",
        "    sample_weights = np.array([weights[int(label)] for label in labels])\n",
        "    logger.info(f\"Sample weights for classes [0, 1]: {weights}\")\n",
        "    return sample_weights\n",
        "\n",
        "def undersample_data(df, high_label=1, low_label=0, ratio=1.5):\n",
        "    \"\"\"Undersample high-empathy class to achieve ~2:1 ratio with low-empathy class\"\"\"\n",
        "    high_df = df[df['labels'] == high_label]\n",
        "    low_df = df[df['labels'] == low_label]\n",
        "    n_low = len(low_df)\n",
        "    n_high = min(len(high_df), n_low * ratio)\n",
        "    high_df = high_df.sample(n=n_high, random_state=42)\n",
        "    balanced_df = pd.concat([high_df, low_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    logger.info(f\"After undersampling: {len(high_df)} high, {len(low_df)} low samples\")\n",
        "    return balanced_df\n",
        "\n",
        "def plot_confusion_matrix(labels, predictions, phase=\"test\", model_name=\"model\"):\n",
        "    \"\"\"Plot and save confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(labels, predictions, labels=[0, 1])\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Low\", \"High\"], yticklabels=[\"Low\", \"High\"])\n",
        "    plt.title(f\"Confusion Matrix ({phase.capitalize()}, {model_name})\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    output_path = os.path.join(\"results\", f\"confusion_matrix_{phase}_{model_name}.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"Confusion matrix saved to {output_path}\")\n",
        "    wandb.log({f\"confusion_matrix_{phase}_{model_name}\": wandb.Image(output_path)})\n",
        "    return cm\n",
        "\n",
        "def plot_f1_scores(labels, predictions, phase=\"test\", model_name=\"model\"):\n",
        "    \"\"\"Plot and save per-class F1 scores\"\"\"\n",
        "    f1 = f1_score(labels, predictions, labels=[0, 1], average=None, zero_division=0)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar([\"Low\", \"High\"], f1, color=\"skyblue\")\n",
        "    plt.title(f\"F1 Scores per Class ({phase.capitalize()}, {model_name})\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    output_path = os.path.join(\"results\", f\"f1_scores_{phase}_{model_name}.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"F1 scores plot saved to {output_path}\")\n",
        "    wandb.log({f\"f1_scores_{phase}_{model_name}\": wandb.Image(output_path)})\n",
        "    return f1\n",
        "\n",
        "def load_and_prepare_data(args, df):\n",
        "    \"\"\"Load and preprocess data for binary classification\"\"\"\n",
        "    df = df.dropna(subset=['response', 'label', 'condition'])\n",
        "    df['label'] = df['label'].astype(float)\n",
        "    if not ((df['label'] >= 0) & (df['label'] <= 9)).all():\n",
        "        logger.warning(\"Some labels are outside [0, 9]. Clipping may affect results.\")\n",
        "    df['labels'] = (df['label'] >= 6).astype(int)\n",
        "    logger.info(f\"Label distribution (binary): {df['labels'].value_counts().to_string()}\")\n",
        "    wandb.log({\"label_distribution\": wandb.Histogram(df['labels'])})\n",
        "\n",
        "    if len(df) < 100:\n",
        "        logger.warning(\"Dataset is small (<100 samples). Consider adding more samples.\")\n",
        "\n",
        "    sample_weights = compute_sample_weights(df['labels'].values)\n",
        "    df['weight'] = sample_weights\n",
        "\n",
        "    df = undersample_data(df, high_label=1, low_label=0, ratio=2)\n",
        "\n",
        "    df['input_text'] = df.apply(lambda x: f\"condition_{int(x['condition'])}: {x['response']}\", axis=1)\n",
        "\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    logger.info(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df[['input_text', 'labels', 'weight']])\n",
        "    test_dataset = Dataset.from_pandas(test_df[['input_text', 'labels']])\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load tokenizer for {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"input_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    if args.max_train_samples != -1:\n",
        "        train_dataset = train_dataset.select(range(min(args.max_train_samples, len(train_dataset))))\n",
        "    if args.max_predict_samples != -1:\n",
        "        test_dataset = test_dataset.select(range(min(args.max_predict_samples, len(test_dataset))))\n",
        "\n",
        "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"weight\"])\n",
        "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    return train_dataset, test_dataset, tokenizer, 1\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute accuracy, F1-score, and confusion matrix\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    logits = logits.squeeze()\n",
        "    predictions = (logits > 0).astype(int)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
        "    f1_per_class = f1_score(labels, predictions, labels=[0, 1], average=None, zero_division=0)\n",
        "    cm = plot_confusion_matrix(labels, predictions, phase=\"test\", model_name=args.model_name.split(\"/\")[-1])\n",
        "    f1_per_class_dict = {f\"f1_class_{i}\": float(f1_per_class[i]) for i in range(2)}\n",
        "    metrics = {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"f1_macro\": float(f1),\n",
        "        **f1_per_class_dict\n",
        "    }\n",
        "    wandb.log(metrics)\n",
        "    return metrics\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"Compute weighted BCE loss\"\"\"\n",
        "        logger.debug(f\"Inputs keys: {list(inputs.keys())}\")\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        weights = inputs.pop(\"weight\", torch.ones_like(labels))\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        # Ensure logits and labels have compatible shapes\n",
        "        if logits.dim() > 1:\n",
        "            logits = logits.squeeze(-1)  # Squeeze only the last dimension if needed\n",
        "        if logits.shape != labels.shape:\n",
        "            logger.debug(f\"Logits shape: {logits.shape}, Labels shape: {labels.shape}\")\n",
        "            logits = logits.view(-1)  # Flatten to [batch_size]\n",
        "        loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        loss = loss_fn(logits, labels.float())\n",
        "        weighted_loss = torch.mean(weights * loss)\n",
        "        return (weighted_loss, outputs) if return_outputs else weighted_loss\n",
        "\n",
        "def train_model(args, train_dataset, test_dataset, num_labels):\n",
        "    \"\"\"Train the model with weighted loss\"\"\"\n",
        "    os.makedirs(args.model_path, exist_ok=True)\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(args.model_path, args.model_name.split(\"/\")[-1]),\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=args.lr,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{args.model_name.split('/')[-1]}_lr_{args.lr}_bs_{args.batch_size}\",\n",
        "        fp16=True,\n",
        "        dataloader_drop_last=True  # Drop incomplete last batch\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=None,\n",
        "        compute_metrics=None\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training failed for {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    trainer.save_model(os.path.join(args.model_path, args.model_name.split(\"/\")[-1]))\n",
        "    return trainer\n",
        "\n",
        "def predict(trainer, test_dataset, args):\n",
        "    \"\"\"Make predictions and compute metrics\"\"\"\n",
        "    try:\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        logits = predictions.predictions.squeeze()\n",
        "        labels = predictions.label_ids\n",
        "        if logits.ndim > 1:\n",
        "            logits = logits[:, 0]\n",
        "        if np.any(np.isnan(logits)):\n",
        "            logger.warning(\"NaN values found in predictions, replacing with 0\")\n",
        "            logits = np.nan_to_num(logits, nan=0.0)\n",
        "        metrics = compute_metrics((logits, labels))\n",
        "        logger.info(f\"Test metrics ({args.model_name}): {metrics}\")\n",
        "        with open(os.path.join(args.model_path, f\"res_{args.model_name.split('/')[-1]}.txt\"), \"a\") as f:\n",
        "            f.write(f\"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}: {metrics}\\n\")\n",
        "        output_path = os.path.join(\"results\", f\"predictions_{args.model_name.split('/')[-1]}.txt\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            for i, logit in enumerate(logits):\n",
        "                pred = 1 if logit > 0 else 0\n",
        "                f.write(f\"{pred}\\n\")\n",
        "                if i % 50 == 0:\n",
        "                    logger.info(f\"Written {i+1} predictions\")\n",
        "        logger.info(f\"Predictions saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during prediction: {e}\")\n",
        "        raise\n",
        "\n",
        "def main(df, model_name=\"bert-base-uncased\"):\n",
        "    \"\"\"Main function to run training and prediction for a given model\"\"\"\n",
        "    global args\n",
        "    args = parse_args()\n",
        "    args.model_name = model_name\n",
        "\n",
        "    try:\n",
        "        wandb.init(project=\"empathy-classification\", name=f\"{model_name.split('/')[-1]}_lr_{args.lr}_bs_{args.batch_size}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to init W&B: {e}. Continuing without W&B.\")\n",
        "        args.report_to = None\n",
        "\n",
        "    try:\n",
        "        train_dataset, test_dataset, tokenizer, num_labels = load_and_prepare_data(args, df)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Data preparation failed for {model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer = train_model(args, train_dataset, test_dataset, num_labels)\n",
        "\n",
        "    if args.do_predict:\n",
        "        if not args.do_train:\n",
        "            try:\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    os.path.join(args.model_path, args.model_name.split(\"/\")[-1]), num_labels=num_labels\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load model for prediction {args.model_name}: {e}\")\n",
        "                raise\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=os.path.join(args.model_path, args.model_name.split(\"/\")[-1]),\n",
        "                per_device_eval_batch_size=args.batch_size,\n",
        "                fp16=True\n",
        "            )\n",
        "            trainer = CustomTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                compute_metrics=None\n",
        "            )\n",
        "        predict(trainer, test_dataset, args)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        df = pd.read_csv('chatbot_responses.csv', sep=',', on_bad_lines='skip', engine='python')\n",
        "        logger.info(f\"Raw CSV loaded with {len(df)} rows\")\n",
        "        data = df[['response', 'label', 'condition']]\n",
        "        if data.empty:\n",
        "            raise ValueError(\"Dataset is empty. Check input data.\")\n",
        "        logger.info(f\"Filtered dataset with {len(data)} rows\")\n",
        "        models = [\n",
        "            # \"bert-base-uncased\"\n",
        "            # \"bert-large-uncased\"\n",
        "            # \"RoBERTa-base\"\n",
        "            # \"DistilBERT-base-uncased\"\n",
        "        ]\n",
        "        for model_name in models:\n",
        "            logger.info(f\"Training and evaluating {model_name}\")\n",
        "            main(data, model_name=model_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "        raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LVfx-zweVFR",
        "outputId": "af4db88d-c075-4557-fb96-3ce9c850834a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ex1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"results\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "iSg9YXFD0_O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save hrel as chatbot_responses.csv\n",
        "import pandas as pd\n",
        "hrel.to_csv('chatbot_responses.csv', index=False)"
      ],
      "metadata": {
        "id": "Zq794keMv-VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bert-base-uncased\n"
      ],
      "metadata": {
        "id": "sgl7C8NQ7YJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DistilBERT-base-uncased\n",
        "!python ex1.py --do_train --do_predict --num_train_epochs 10 --lr 1e-5 --batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1kQyC4-7Fn6",
        "outputId": "b41e7473-f027-41bb-8ce0-6eb3a9aeb9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-23 07:55:31.538551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755935731.598388   14711 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755935731.609107   14711 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755935731.641163   14711 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755935731.641240   14711 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755935731.641252   14711 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755935731.641260   14711 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshachar-ashkenazi\u001b[0m (\u001b[33mcp_ofri\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250823_075542-i2sy9hkb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mDistilBERT-base-uncased_lr_1e-05_bs_16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification/runs/i2sy9hkb\u001b[0m\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 89.7kB/s]\n",
            "config.json: 100% 483/483 [00:00<00:00, 1.92MB/s]\n",
            "vocab.txt: 232kB [00:00, 16.8MB/s]\n",
            "tokenizer.json: 466kB [00:00, 63.9MB/s]\n",
            "Map: 100% 513/513 [00:00<00:00, 2131.89 examples/s]\n",
            "Map: 100% 129/129 [00:00<00:00, 2134.91 examples/s]\n",
            "model.safetensors: 100% 268M/268M [00:05<00:00, 45.6MB/s]\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at DistilBERT-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "{'loss': 0.6723, 'grad_norm': 3.0142948627471924, 'learning_rate': 9.71875e-06, 'epoch': 0.31}\n",
            "{'loss': 0.6448, 'grad_norm': 1.6527684926986694, 'learning_rate': 9.406250000000002e-06, 'epoch': 0.62}\n",
            "{'loss': 0.6686, 'grad_norm': 0.7794310450553894, 'learning_rate': 9.09375e-06, 'epoch': 0.94}\n",
            "{'loss': 0.6497, 'grad_norm': 1.1411458253860474, 'learning_rate': 8.781250000000002e-06, 'epoch': 1.25}\n",
            "{'loss': 0.5929, 'grad_norm': 2.4492974281311035, 'learning_rate': 8.468750000000001e-06, 'epoch': 1.56}\n",
            "{'loss': 0.7072, 'grad_norm': 2.210118055343628, 'learning_rate': 8.156250000000002e-06, 'epoch': 1.88}\n",
            "{'loss': 0.6312, 'grad_norm': 0.7064208984375, 'learning_rate': 7.843750000000001e-06, 'epoch': 2.19}\n",
            "{'loss': 0.6054, 'grad_norm': 1.4490338563919067, 'learning_rate': 7.531250000000001e-06, 'epoch': 2.5}\n",
            "{'loss': 0.6666, 'grad_norm': 4.040492057800293, 'learning_rate': 7.218750000000001e-06, 'epoch': 2.81}\n",
            "{'loss': 0.6199, 'grad_norm': 1.764784812927246, 'learning_rate': 6.906250000000001e-06, 'epoch': 3.12}\n",
            "{'loss': 0.6123, 'grad_norm': 1.055242657661438, 'learning_rate': 6.593750000000001e-06, 'epoch': 3.44}\n",
            "{'loss': 0.6367, 'grad_norm': 1.2098848819732666, 'learning_rate': 6.281250000000001e-06, 'epoch': 3.75}\n",
            "{'loss': 0.6095, 'grad_norm': 1.8280940055847168, 'learning_rate': 5.968750000000001e-06, 'epoch': 4.06}\n",
            "{'loss': 0.6135, 'grad_norm': 1.2916173934936523, 'learning_rate': 5.656250000000001e-06, 'epoch': 4.38}\n",
            "{'loss': 0.5969, 'grad_norm': 1.4329272508621216, 'learning_rate': 5.343750000000001e-06, 'epoch': 4.69}\n",
            "{'loss': 0.5935, 'grad_norm': 2.2504360675811768, 'learning_rate': 5.031250000000001e-06, 'epoch': 5.0}\n",
            "{'loss': 0.5868, 'grad_norm': 2.7134554386138916, 'learning_rate': 4.71875e-06, 'epoch': 5.31}\n",
            "{'loss': 0.5755, 'grad_norm': 2.0969793796539307, 'learning_rate': 4.40625e-06, 'epoch': 5.62}\n",
            "{'loss': 0.5622, 'grad_norm': 2.191232442855835, 'learning_rate': 4.09375e-06, 'epoch': 5.94}\n",
            "{'loss': 0.5739, 'grad_norm': 2.385882616043091, 'learning_rate': 3.78125e-06, 'epoch': 6.25}\n",
            "{'loss': 0.5359, 'grad_norm': 2.663133382797241, 'learning_rate': 3.46875e-06, 'epoch': 6.56}\n",
            "{'loss': 0.5941, 'grad_norm': 2.3718855381011963, 'learning_rate': 3.15625e-06, 'epoch': 6.88}\n",
            "{'loss': 0.4974, 'grad_norm': 3.0944111347198486, 'learning_rate': 2.84375e-06, 'epoch': 7.19}\n",
            "{'loss': 0.561, 'grad_norm': 2.0533971786499023, 'learning_rate': 2.53125e-06, 'epoch': 7.5}\n",
            "{'loss': 0.5412, 'grad_norm': 3.3503153324127197, 'learning_rate': 2.21875e-06, 'epoch': 7.81}\n",
            "{'loss': 0.5664, 'grad_norm': 3.1862452030181885, 'learning_rate': 1.90625e-06, 'epoch': 8.12}\n",
            "{'loss': 0.5021, 'grad_norm': 4.691804885864258, 'learning_rate': 1.59375e-06, 'epoch': 8.44}\n",
            "{'loss': 0.4945, 'grad_norm': 2.011575222015381, 'learning_rate': 1.28125e-06, 'epoch': 8.75}\n",
            "{'loss': 0.5605, 'grad_norm': 4.117865085601807, 'learning_rate': 9.6875e-07, 'epoch': 9.06}\n",
            "{'loss': 0.4699, 'grad_norm': 1.8963403701782227, 'learning_rate': 6.562500000000001e-07, 'epoch': 9.38}\n",
            "{'loss': 0.4914, 'grad_norm': 2.097081184387207, 'learning_rate': 3.4375000000000004e-07, 'epoch': 9.69}\n",
            "{'loss': 0.5566, 'grad_norm': 2.9227166175842285, 'learning_rate': 3.1250000000000005e-08, 'epoch': 10.0}\n",
            "{'train_runtime': 171.951, 'train_samples_per_second': 29.834, 'train_steps_per_second': 1.861, 'train_loss': 0.587201863527298, 'epoch': 10.0}\n",
            "100% 320/320 [02:51<00:00,  1.86it/s]\n",
            "100% 8/8 [00:00<00:00, 19.78it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_class_0 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_class_1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/grad_norm ▅▃▁▂▄▄▁▂▇▃▂▂▃▂▂▄▅▃▄▄▄▄▅▃▆▅█▃▇▃▃▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss ▇▆▇▆▅█▆▅▇▅▅▆▅▅▅▅▄▄▄▄▃▅▂▄▃▄▂▂▄▁▂▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 accuracy 0.72656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               f1_class_0 0.14634\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               f1_class_1 0.83721\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 f1_macro 0.49178\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                test/loss 0.60085\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/runtime 0.4856\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test/samples_per_second 265.656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/steps_per_second 18.534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 678220985794560.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 320\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 2.92272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.5566\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.5872\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 171.951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 29.834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 1.861\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mDistilBERT-base-uncased_lr_1e-05_bs_16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification/runs/i2sy9hkb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250823_075542-i2sy9hkb/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RoBERTa-base\n",
        "!python ex1.py --do_train --do_predict --num_train_epochs 10 --lr 1e-5 --batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMiiCSqgiYMT",
        "outputId": "99d6aadd-7821-4e66-e2d2-e213ca629153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-23 07:38:56.012336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1755934736.032990   10534 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1755934736.039452   10534 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1755934736.055329   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755934736.055356   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755934736.055360   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1755934736.055366   10534 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshachar-ashkenazi\u001b[0m (\u001b[33mcp_ofri\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250823_073900-ejszpsmb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mRoBERTa-base_lr_1e-05_bs_16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification/runs/ejszpsmb\u001b[0m\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 110kB/s]\n",
            "config.json: 100% 481/481 [00:00<00:00, 1.19MB/s]\n",
            "vocab.json: 899kB [00:00, 39.6MB/s]\n",
            "merges.txt: 456kB [00:00, 79.8MB/s]\n",
            "tokenizer.json: 1.36MB [00:00, 101MB/s]\n",
            "Map: 100% 513/513 [00:00<00:00, 2260.00 examples/s]\n",
            "Map: 100% 129/129 [00:00<00:00, 2282.50 examples/s]\n",
            "model.safetensors: 100% 499M/499M [00:06<00:00, 75.3MB/s]\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at RoBERTa-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "{'loss': 0.6563, 'grad_norm': 3.7091217041015625, 'learning_rate': 9.71875e-06, 'epoch': 0.31}\n",
            "{'loss': 0.6565, 'grad_norm': 3.846588134765625, 'learning_rate': 9.406250000000002e-06, 'epoch': 0.62}\n",
            "{'loss': 0.6806, 'grad_norm': 1.8004090785980225, 'learning_rate': 9.09375e-06, 'epoch': 0.94}\n",
            "{'loss': 0.6612, 'grad_norm': 2.4084744453430176, 'learning_rate': 8.781250000000002e-06, 'epoch': 1.25}\n",
            "{'loss': 0.6033, 'grad_norm': 4.656250476837158, 'learning_rate': 8.468750000000001e-06, 'epoch': 1.56}\n",
            "{'loss': 0.7051, 'grad_norm': inf, 'learning_rate': 8.156250000000002e-06, 'epoch': 1.88}\n",
            "{'loss': 0.6308, 'grad_norm': 2.432905435562134, 'learning_rate': 7.843750000000001e-06, 'epoch': 2.19}\n",
            "{'loss': 0.6188, 'grad_norm': 2.3069262504577637, 'learning_rate': 7.531250000000001e-06, 'epoch': 2.5}\n",
            "{'loss': 0.6761, 'grad_norm': 6.655669212341309, 'learning_rate': 7.218750000000001e-06, 'epoch': 2.81}\n",
            "{'loss': 0.628, 'grad_norm': 4.641149044036865, 'learning_rate': 6.906250000000001e-06, 'epoch': 3.12}\n",
            "{'loss': 0.6173, 'grad_norm': 3.784144401550293, 'learning_rate': 6.593750000000001e-06, 'epoch': 3.44}\n",
            "{'loss': 0.6692, 'grad_norm': 4.091928005218506, 'learning_rate': 6.281250000000001e-06, 'epoch': 3.75}\n",
            "{'loss': 0.6087, 'grad_norm': 2.983790397644043, 'learning_rate': 5.968750000000001e-06, 'epoch': 4.06}\n",
            "{'loss': 0.623, 'grad_norm': 3.3976564407348633, 'learning_rate': 5.656250000000001e-06, 'epoch': 4.38}\n",
            "{'loss': 0.5977, 'grad_norm': 3.9706215858459473, 'learning_rate': 5.343750000000001e-06, 'epoch': 4.69}\n",
            "{'loss': 0.604, 'grad_norm': 6.927133560180664, 'learning_rate': 5.031250000000001e-06, 'epoch': 5.0}\n",
            "{'loss': 0.6025, 'grad_norm': 4.187129497528076, 'learning_rate': 4.71875e-06, 'epoch': 5.31}\n",
            "{'loss': 0.595, 'grad_norm': 10.392851829528809, 'learning_rate': 4.40625e-06, 'epoch': 5.62}\n",
            "{'loss': 0.5959, 'grad_norm': 6.6117658615112305, 'learning_rate': 4.09375e-06, 'epoch': 5.94}\n",
            "{'loss': 0.5881, 'grad_norm': 7.103279113769531, 'learning_rate': 3.78125e-06, 'epoch': 6.25}\n",
            "{'loss': 0.5382, 'grad_norm': 5.303792476654053, 'learning_rate': 3.46875e-06, 'epoch': 6.56}\n",
            "{'loss': 0.6158, 'grad_norm': 5.12309455871582, 'learning_rate': 3.15625e-06, 'epoch': 6.88}\n",
            "{'loss': 0.4721, 'grad_norm': 7.503380298614502, 'learning_rate': 2.84375e-06, 'epoch': 7.19}\n",
            "{'loss': 0.5702, 'grad_norm': 6.70798397064209, 'learning_rate': 2.53125e-06, 'epoch': 7.5}\n",
            "{'loss': 0.5509, 'grad_norm': 6.173105716705322, 'learning_rate': 2.21875e-06, 'epoch': 7.81}\n",
            "{'loss': 0.5687, 'grad_norm': 6.493465900421143, 'learning_rate': 1.90625e-06, 'epoch': 8.12}\n",
            "{'loss': 0.4559, 'grad_norm': 11.749167442321777, 'learning_rate': 1.59375e-06, 'epoch': 8.44}\n",
            "{'loss': 0.5046, 'grad_norm': 13.371602058410645, 'learning_rate': 1.28125e-06, 'epoch': 8.75}\n",
            "{'loss': 0.5231, 'grad_norm': 13.094613075256348, 'learning_rate': 9.6875e-07, 'epoch': 9.06}\n",
            "{'loss': 0.447, 'grad_norm': 15.118335723876953, 'learning_rate': 6.562500000000001e-07, 'epoch': 9.38}\n",
            "{'loss': 0.4751, 'grad_norm': 16.036134719848633, 'learning_rate': 3.4375000000000004e-07, 'epoch': 9.69}\n",
            "{'loss': 0.5296, 'grad_norm': 8.278337478637695, 'learning_rate': 3.1250000000000005e-08, 'epoch': 10.0}\n",
            "{'train_runtime': 336.3679, 'train_samples_per_second': 15.251, 'train_steps_per_second': 0.951, 'train_loss': 0.5896726727485657, 'epoch': 10.0}\n",
            "100% 320/320 [05:36<00:00,  1.05s/it]\n",
            "100% 8/8 [00:00<00:00, 10.67it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_class_0 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_class_1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_macro ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/grad_norm ▂▂▁▁▂ ▁▁▃▂▂▂▂▂▂▄▂▅▃▄▃▃▄▃▃▃▆▇▇██▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss ▇▇▇▇▅█▆▆▇▆▆▇▅▆▅▅▅▅▅▅▃▆▂▄▄▄▁▃▃▁▂▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 accuracy 0.71094\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               f1_class_0 0.05128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               f1_class_1 0.82949\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 f1_macro 0.44039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                test/loss 0.6525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/runtime 0.9022\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test/samples_per_second 142.989\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/steps_per_second 9.976\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 1347116508119040.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 320\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 8.27834\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 0.5296\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 0.58967\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 336.3679\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 15.251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 0.951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mRoBERTa-base_lr_1e-05_bs_16\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification/runs/ejszpsmb\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/cp_ofri/empathy-classification\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250823_073900-ejszpsmb/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine tuning with regression"
      ],
      "metadata": {
        "id": "rN13m_rgWy0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFgIvgVfqxbo",
        "outputId": "ab70295f-73fc-47a5-b7f8-b5de9c52a3cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting regression.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile regression.py\n",
        "\n",
        "\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Fine-tune models for binary empathy classification with weighted loss\")\n",
        "    parser.add_argument(\"--max_train_samples\", type=int, default=-1, help=\"Number of training samples or -1 for all\")\n",
        "    parser.add_argument(\"--max_predict_samples\", type=int, default=-1, help=\"Number of test samples or -1 for all\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for training and evaluation\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Run training\")\n",
        "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Run prediction\")\n",
        "    parser.add_argument(\"--model_path\", type=str, default=\"./results\", help=\"Path to save/load model\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default=\"bert-base-uncased\", help=\"Model name (bert-base-uncased, roberta-base, distilbert-base-uncased, electra-small-discriminator)\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def compute_sample_weights(labels):\n",
        "    \"\"\"Compute sample weights for binary classification based on class frequency\"\"\"\n",
        "    bins = np.array([0, 1, 2])  # Bins for classes 0 and 1\n",
        "    freq, _ = np.histogram(labels, bins=bins)\n",
        "    freq = freq + 1e-6  # Avoid division by zero\n",
        "    weights = np.max(freq) / freq * 2.0\n",
        "    sample_weights = np.array([weights[int(label)] for label in labels])\n",
        "    logger.info(f\"Sample weights for classes [0, 1]: {weights}\")\n",
        "    return sample_weights\n",
        "\n",
        "def undersample_data(df, high_label=1, low_label=0, ratio=1):\n",
        "    \"\"\"Undersample high-empathy class to achieve ~2:1 ratio with low-empathy class\"\"\"\n",
        "    high_df = df[df['labels'] == high_label]\n",
        "    low_df = df[df['labels'] == low_label]\n",
        "    n_low = len(low_df)\n",
        "    n_high = min(len(high_df), n_low * ratio)\n",
        "    high_df = high_df.sample(n=n_high, random_state=42)\n",
        "    balanced_df = pd.concat([high_df, low_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    logger.info(f\"After undersampling: {len(high_df)} high, {len(low_df)} low samples\")\n",
        "    return balanced_df\n",
        "\n",
        "def plot_confusion_matrix(labels, predictions, phase=\"test\", model_name=\"model\"):\n",
        "    \"\"\"Plot and save confusion matrix\"\"\"\n",
        "    cm = confusion_matrix(labels, predictions, labels=[0, 1])\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Low\", \"High\"], yticklabels=[\"Low\", \"High\"])\n",
        "    plt.title(f\"Confusion Matrix ({phase.capitalize()}, {model_name})\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    output_path = os.path.join(\"results\", f\"confusion_matrix_{phase}_{model_name}.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"Confusion matrix saved to {output_path}\")\n",
        "    wandb.log({f\"confusion_matrix_{phase}_{model_name}\": wandb.Image(output_path)})\n",
        "    return cm\n",
        "\n",
        "def plot_f1_scores(labels, predictions, phase=\"test\", model_name=\"model\"):\n",
        "    \"\"\"Plot and save per-class F1 scores\"\"\"\n",
        "    f1 = f1_score(labels, predictions, labels=[0, 1], average=None, zero_division=0)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.bar([\"Low\", \"High\"], f1, color=\"skyblue\")\n",
        "    plt.title(f\"F1 Scores per Class ({phase.capitalize()}, {model_name})\")\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.ylim(0, 1)\n",
        "    output_path = os.path.join(\"results\", f\"f1_scores_{phase}_{model_name}.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"F1 scores plot saved to {output_path}\")\n",
        "    wandb.log({f\"f1_scores_{phase}_{model_name}\": wandb.Image(output_path)})\n",
        "    return f1\n",
        "\n",
        "def load_and_prepare_data(args, df):\n",
        "    \"\"\"Load and preprocess data for binary classification\"\"\"\n",
        "    df = df.dropna(subset=['response', 'label', 'condition'])\n",
        "    df['label'] = df['label'].astype(float)\n",
        "    if not ((df['label'] >= 0) & (df['label'] <= 9)).all():\n",
        "        logger.warning(\"Some labels are outside [0, 9]. Clipping may affect results.\")\n",
        "    df['labels'] = (df['label'] >= 6).astype(int)\n",
        "    logger.info(f\"Label distribution (binary): {df['labels'].value_counts().to_string()}\")\n",
        "    wandb.log({\"label_distribution\": wandb.Histogram(df['labels'])})\n",
        "\n",
        "\n",
        "    sample_weights = compute_sample_weights(df['labels'].values)\n",
        "    df['weight'] = sample_weights\n",
        "\n",
        "\n",
        "    df['input_text'] = df.apply(lambda x: f\"condition_{int(x['condition'])}: {x['response']}\", axis=1)\n",
        "\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    train_df = undersample_data(train_df, high_label=1, low_label=0, ratio=1)\n",
        "\n",
        "    logger.info(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df[['input_text', 'labels', 'weight']])\n",
        "    test_dataset = Dataset.from_pandas(test_df[['input_text', 'labels']])\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load tokenizer for {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"input_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    if args.max_train_samples != -1:\n",
        "        train_dataset = train_dataset.select(range(min(args.max_train_samples, len(train_dataset))))\n",
        "    if args.max_predict_samples != -1:\n",
        "        test_dataset = test_dataset.select(range(min(args.max_predict_samples, len(test_dataset))))\n",
        "\n",
        "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"weight\"])\n",
        "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    return train_dataset, test_dataset, tokenizer, 1\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute accuracy, F1-score, and confusion matrix\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    logits = logits.squeeze()\n",
        "    predictions = (logits > 0).astype(int)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
        "    f1_per_class = f1_score(labels, predictions, labels=[0, 1], average=None, zero_division=0)\n",
        "    cm = plot_confusion_matrix(labels, predictions, phase=\"test\", model_name=args.model_name.split(\"/\")[-1])\n",
        "    f1_per_class_dict = {f\"f1_class_{i}\": float(f1_per_class[i]) for i in range(2)}\n",
        "    metrics = {\n",
        "        \"accuracy\": float(acc),\n",
        "        \"f1_macro\": float(f1),\n",
        "        **f1_per_class_dict\n",
        "    }\n",
        "    wandb.log(metrics)\n",
        "    return metrics\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"Compute weighted BCE loss\"\"\"\n",
        "        logger.debug(f\"Inputs keys: {list(inputs.keys())}\")\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        weights = inputs.pop(\"weight\", torch.ones_like(labels))\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        # Ensure logits and labels have compatible shapes\n",
        "        if logits.dim() > 1:\n",
        "            logits = logits.squeeze(-1)  # Squeeze only the last dimension if needed\n",
        "        if logits.shape != labels.shape:\n",
        "            logger.debug(f\"Logits shape: {logits.shape}, Labels shape: {labels.shape}\")\n",
        "            logits = logits.view(-1)  # Flatten to [batch_size]\n",
        "        loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "        loss = loss_fn(logits, labels.float())\n",
        "        weighted_loss = torch.mean(weights * loss)\n",
        "        return (weighted_loss, outputs) if return_outputs else weighted_loss\n",
        "\n",
        "def train_model(args, train_dataset, test_dataset, num_labels):\n",
        "    \"\"\"Train the model with weighted loss\"\"\"\n",
        "    os.makedirs(args.model_path, exist_ok=True)\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(args.model_path, args.model_name.split(\"/\")[-1]),\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=args.lr,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{args.model_name.split('/')[-1]}_lr_{args.lr}_bs_{args.batch_size}\",\n",
        "        fp16=True,\n",
        "        dataloader_drop_last=True  # Drop incomplete last batch\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=None,\n",
        "        compute_metrics=None\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training failed for {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    trainer.save_model(os.path.join(args.model_path, args.model_name.split(\"/\")[-1]))\n",
        "    return trainer\n",
        "\n",
        "def predict(trainer, test_dataset, args):\n",
        "    \"\"\"Make predictions and compute metrics\"\"\"\n",
        "    try:\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        logits = predictions.predictions.squeeze()\n",
        "        labels = predictions.label_ids\n",
        "        if logits.ndim > 1:\n",
        "            logits = logits[:, 0]\n",
        "        if np.any(np.isnan(logits)):\n",
        "            logger.warning(\"NaN values found in predictions, replacing with 0\")\n",
        "            logits = np.nan_to_num(logits, nan=0.0)\n",
        "        metrics = compute_metrics((logits, labels))\n",
        "        logger.info(f\"Test metrics ({args.model_name}): {metrics}\")\n",
        "        with open(os.path.join(args.model_path, f\"res_{args.model_name.split('/')[-1]}.txt\"), \"a\") as f:\n",
        "            f.write(f\"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}: {metrics}\\n\")\n",
        "        output_path = os.path.join(\"results\", f\"predictions_{args.model_name.split('/')[-1]}.txt\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            for i, logit in enumerate(logits):\n",
        "                pred = 1 if logit > 0 else 0\n",
        "                f.write(f\"{pred}\\n\")\n",
        "                if i % 50 == 0:\n",
        "                    logger.info(f\"Written {i+1} predictions\")\n",
        "        logger.info(f\"Predictions saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during prediction: {e}\")\n",
        "        raise\n",
        "\n",
        "def main(df, model_name=\"bert-base-uncased\"):\n",
        "    \"\"\"Main function to run training and prediction for a given model\"\"\"\n",
        "    global args\n",
        "    args = parse_args()\n",
        "    args.model_name = model_name\n",
        "\n",
        "    try:\n",
        "        wandb.init(project=\"empathy-classification\", name=f\"{model_name.split('/')[-1]}_lr_{args.lr}_bs_{args.batch_size}\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to init W&B: {e}. Continuing without W&B.\")\n",
        "        args.report_to = None\n",
        "\n",
        "    try:\n",
        "        train_dataset, test_dataset, tokenizer, num_labels = load_and_prepare_data(args, df)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Data preparation failed for {model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer = train_model(args, train_dataset, test_dataset, num_labels)\n",
        "\n",
        "    if args.do_predict:\n",
        "        if not args.do_train:\n",
        "            try:\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    os.path.join(args.model_path, args.model_name.split(\"/\")[-1]), num_labels=num_labels\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to load model for prediction {args.model_name}: {e}\")\n",
        "                raise\n",
        "            training_args = TrainingArguments(\n",
        "                output_dir=os.path.join(args.model_path, args.model_name.split(\"/\")[-1]),\n",
        "                per_device_eval_batch_size=args.batch_size,\n",
        "                fp16=True\n",
        "            )\n",
        "            trainer = CustomTrainer(\n",
        "                model=model,\n",
        "                args=training_args,\n",
        "                compute_metrics=None\n",
        "            )\n",
        "        predict(trainer, test_dataset, args)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = pd.read_csv('chatbot_responses.csv', sep=',', on_bad_lines='skip', engine='python')\n",
        "        logger.info(f\"Raw CSV loaded with {len(df)} rows\")\n",
        "        data = df[['response', 'label', 'condition']]\n",
        "        if data.empty:\n",
        "            raise ValueError(\"Dataset is empty. Check input data.\")\n",
        "        logger.info(f\"Filtered dataset with {len(data)} rows\")\n",
        "        models = [\n",
        "            \"bert-base-uncased\",\n",
        "        ]\n",
        "        for model_name in models:\n",
        "            logger.info(f\"Training and evaluating {model_name}\")\n",
        "            main(data, model_name=model_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile regression2.py\n",
        "import argparse\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, confusion_matrix\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Evaluate roberta-base regression model with undersampling and confusion matrix\")\n",
        "    parser.add_argument(\"--max_train_samples\", type=int, default=-1, help=\"Number of training samples or -1 for all\")\n",
        "    parser.add_argument(\"--max_predict_samples\", type=int, default=-1, help=\"Number of test samples or -1 for all\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=3, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=2e-5, help=\"Learning rate\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for training and evaluation\")\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Run training if model not found\")\n",
        "    parser.add_argument(\"--do_predict\", action=\"store_true\", help=\"Run prediction\")\n",
        "    parser.add_argument(\"--model_path\", type=str, default=\"./results_regression\", help=\"Path to save/load model\")\n",
        "    parser.add_argument(\"--model_name\", type=str, default=\"roberta-base\", help=\"Model name\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def compute_sample_weights(labels):\n",
        "    \"\"\"Compute sample weights for regression based on label frequency\"\"\"\n",
        "    bins = np.arange(0, 11)  # Bins for labels 0–9\n",
        "    freq, _ = np.histogram(labels, bins=bins)\n",
        "    freq = freq + 1e-6  # Avoid division by zero\n",
        "    weights = np.max(freq) / freq * 2.0  # 2x amplification for rare classes\n",
        "    sample_weights = np.array([weights[int(label)] for label in labels])\n",
        "    logger.info(f\"Sample weights for labels 0–9: {weights}\")\n",
        "    return sample_weights\n",
        "\n",
        "def undersample_data(df, majority_labels=[7, 8, 9], ratio=2):\n",
        "    \"\"\"Undersample majority labels (7–9) to ~3:1 ratio with the rarest label\"\"\"\n",
        "    label_counts = df['labels'].value_counts().to_dict()\n",
        "    minority_counts = {k: v for k, v in label_counts.items() if k < 7}\n",
        "\n",
        "    min_count = min(minority_counts.values())\n",
        "    target_count = int(min_count * ratio)\n",
        "    logger.info(f\"Rarest label count: {min_count}, Target count for majority labels: {target_count}\")\n",
        "\n",
        "    dfs = []\n",
        "    for label in range(10):\n",
        "        label_df = df[df['labels'] == label]\n",
        "        if label in majority_labels and len(label_df) > target_count:\n",
        "            label_df = label_df.sample(n=target_count, random_state=42)\n",
        "            logger.info(f\"Undersampled label {label} from {label_counts.get(label, 0)} to {target_count}\")\n",
        "        dfs.append(label_df)\n",
        "\n",
        "    balanced_df = pd.concat(dfs).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "    logger.info(f\"After undersampling: {len(balanced_df)} samples, Label distribution: {balanced_df['labels'].value_counts().sort_index().to_string()}\")\n",
        "    return balanced_df\n",
        "\n",
        "def plot_mae_per_score(labels, predictions, phase=\"test\", model_name=\"model\"):\n",
        "    \"\"\"Plot and save MAE per score (0–9) with sample counts\"\"\"\n",
        "    mae_per_score = []\n",
        "    sample_counts = []\n",
        "    for score in range(10):\n",
        "        mask = labels == score\n",
        "        count = np.sum(mask)\n",
        "        sample_counts.append(count)\n",
        "        if count > 0:\n",
        "            mae = mean_absolute_error(labels[mask], predictions[mask])\n",
        "        else:\n",
        "            mae = 0.0\n",
        "        mae_per_score.append(mae)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(range(10), mae_per_score, color=\"skyblue\")\n",
        "    plt.title(f\"MAE per Empathy Score ({phase.capitalize()}, {model_name})\")\n",
        "    plt.xlabel(\"Empathy Score\")\n",
        "    plt.ylabel(\"Mean Absolute Error\")\n",
        "    plt.ylim(0, max(mae_per_score + [1]) * 1.2)\n",
        "    for i, (mae, count) in enumerate(zip(mae_per_score, sample_counts)):\n",
        "        plt.text(i, mae + 0.05, f\"n={count}\", ha=\"center\", fontsize=8)\n",
        "    output_path = os.path.join(\"results_regression\", f\"mae_per_score_{phase}_{model_name}.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"MAE per score plot saved to {output_path}\")\n",
        "    wandb.log({f\"mae_per_score_{phase}_{model_name}\": wandb.Image(output_path)})\n",
        "    return mae_per_score, sample_counts\n",
        "\n",
        "def plot_confusion_matrix(labels, predictions, phase=\"test\", model_name=\"model\"):\n",
        "    \"\"\"Plot and save confusion matrix for rounded predictions\"\"\"\n",
        "    cm = confusion_matrix(labels, predictions, labels=range(10))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
        "    plt.title(f\"Confusion Matrix ({phase.capitalize()}, {model_name})\")\n",
        "    plt.xlabel(\"Predicted Score\")\n",
        "    plt.ylabel(\"True Score\")\n",
        "    output_path = os.path.join(\"results_regression\", f\"confusion_matrix_{phase}_{model_name}.png\")\n",
        "    plt.savefig(output_path)\n",
        "    plt.close()\n",
        "    logger.info(f\"Confusion matrix saved to {output_path}\")\n",
        "    wandb.log({f\"confusion_matrix_{phase}_{model_name}\": wandb.Image(output_path)})\n",
        "    return cm\n",
        "\n",
        "def load_and_prepare_data(args, df):\n",
        "    \"\"\"Load and preprocess data for regression with undersampling\"\"\"\n",
        "    df = df.dropna(subset=['response', 'label', 'condition'])\n",
        "    df['labels'] = df['label'].astype(float)\n",
        "    if not ((df['labels'] >= 0) & (df['labels'] <= 9)).all():\n",
        "        logger.warning(\"Some labels are outside [0, 9]. Clipping may affect results.\")\n",
        "    logger.info(f\"Original label distribution: {df['labels'].value_counts().sort_index().to_string()}\")\n",
        "\n",
        "\n",
        "    df['input_text'] = df.apply(lambda x: f\"condition_{int(x['condition'])}: {x['response']}\", axis=1)\n",
        "\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    train_df = undersample_data(train_df, majority_labels=[7, 8, 9], ratio=2)\n",
        "\n",
        "    sample_weights = compute_sample_weights(train_df['labels'].values)\n",
        "    train_df['weight'] = sample_weights\n",
        "\n",
        "    logger.info(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n",
        "    logger.info(f\"Test label distribution: {test_df['labels'].value_counts().sort_index().to_string()}\")\n",
        "\n",
        "    train_dataset = Dataset.from_pandas(train_df[['input_text', 'labels', 'weight']])\n",
        "    test_dataset = Dataset.from_pandas(test_df[['input_text', 'labels']])\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load tokenizer for {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"input_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    if args.max_train_samples != -1:\n",
        "        train_dataset = train_dataset.select(range(min(args.max_train_samples, len(train_dataset))))\n",
        "    if args.max_predict_samples != -1:\n",
        "        test_dataset = test_dataset.select(range(min(args.max_predict_samples, len(test_dataset))))\n",
        "\n",
        "    train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"weight\"])\n",
        "    test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    return train_dataset, test_dataset, tokenizer, 1\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute MSE, MAE, per-score MAE, and confusion matrix\"\"\"\n",
        "    global args\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = predictions.squeeze()\n",
        "    if np.any(np.isnan(predictions)):\n",
        "        logger.warning(\"NaN values found in predictions, replacing with 0\")\n",
        "        predictions = np.nan_to_num(predictions, nan=0.0)\n",
        "    pred_scores_rounded = np.round(predictions).clip(0, 9).astype(int)\n",
        "    mse = mean_squared_error(labels, predictions)\n",
        "    mae = mean_absolute_error(labels, predictions)\n",
        "    mae_per_score, sample_counts = plot_mae_per_score(labels, predictions, phase=\"test\", model_name=args.model_name.split(\"/\")[-1])\n",
        "    cm = plot_confusion_matrix(labels, pred_scores_rounded, phase=\"test\", model_name=args.model_name.split(\"/\")[-1])\n",
        "    metrics = {\n",
        "        \"mse\": float(mse),\n",
        "        \"mae\": float(mae),\n",
        "        **{f\"mae_score_{i}\": float(mae_per_score[i]) for i in range(10)},\n",
        "        **{f\"sample_count_score_{i}\": int(sample_counts[i]) for i in range(10)}\n",
        "    }\n",
        "    wandb.log(metrics)\n",
        "    return metrics\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        \"\"\"Compute weighted MSE loss\"\"\"\n",
        "        logger.debug(f\"Inputs keys: {list(inputs.keys())}\")\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        weights = inputs.pop(\"weight\", torch.ones_like(labels))\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.squeeze()\n",
        "        loss_fn = nn.MSELoss(reduction='none')\n",
        "        loss = loss_fn(predictions, labels.float())\n",
        "        weighted_loss = torch.mean(weights * loss)\n",
        "        return (weighted_loss, outputs) if return_outputs else weighted_loss\n",
        "\n",
        "def train_model(args, train_dataset, test_dataset, num_labels):\n",
        "    \"\"\"Train the model with weighted MSE loss\"\"\"\n",
        "    os.makedirs(args.model_path, exist_ok=True)\n",
        "    try:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=num_labels)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to load model {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(args.model_path, args.model_name.split(\"/\")[-1]),\n",
        "        eval_strategy=\"no\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=args.lr,\n",
        "        per_device_train_batch_size=args.batch_size,\n",
        "        per_device_eval_batch_size=args.batch_size,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        logging_steps=10,\n",
        "        save_total_limit=2,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{args.model_name.split('/')[-1]}_lr_{args.lr}_bs_{args.batch_size}_undersampled\",\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=None,\n",
        "        compute_metrics=None\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training failed for {args.model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    trainer.save_model(os.path.join(args.model_path, args.model_name.split(\"/\")[-1]))\n",
        "    return trainer\n",
        "\n",
        "def predict(trainer, test_dataset, args):\n",
        "    \"\"\"Make predictions and compute metrics\"\"\"\n",
        "    try:\n",
        "        predictions = trainer.predict(test_dataset)\n",
        "        pred_scores = predictions.predictions.squeeze()\n",
        "        labels = predictions.label_ids\n",
        "        if pred_scores.ndim > 1:\n",
        "            pred_scores = pred_scores[:, 0]\n",
        "        if np.any(np.isnan(pred_scores)):\n",
        "            logger.warning(\"NaN values found in predictions, replacing with 0\")\n",
        "            pred_scores = np.nan_to_num(pred_scores, nan=0.0)\n",
        "        pred_scores_rounded = np.round(pred_scores).clip(0, 9).astype(int)\n",
        "        metrics = compute_metrics((pred_scores, labels))\n",
        "        logger.info(f\"Test metrics ({args.model_name}): {metrics}\")\n",
        "        with open(os.path.join(args.model_path, f\"res_{args.model_name.split('/')[-1]}_undersampled.txt\"), \"a\") as f:\n",
        "            f.write(f\"lr_{args.lr}_bs_{args.batch_size}_epochs_{args.num_train_epochs}: {metrics}\\n\")\n",
        "        output_path = os.path.join(\"results_regression\", f\"predictions_{args.model_name.split('/')[-1]}_undersampled.txt\")\n",
        "        with open(output_path, \"w\") as f:\n",
        "            for i, (score, rounded_score) in enumerate(zip(pred_scores, pred_scores_rounded)):\n",
        "                f.write(f\"Raw: {score:.4f}, Rounded: {int(rounded_score)}\\n\")\n",
        "                if i % 50 == 0:\n",
        "                    logger.info(f\"Written {i+1} predictions\")\n",
        "        logger.info(f\"Predictions saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during prediction: {e}\")\n",
        "        raise\n",
        "\n",
        "def main(df, model_name=\"bert-base-uncased\"):\n",
        "    \"\"\"Main function to run training and prediction with undersampling and confusion matrix\"\"\"\n",
        "    global args\n",
        "    args = parse_args()\n",
        "    args.model_name = model_name\n",
        "\n",
        "    try:\n",
        "        wandb.init(project=\"empathy-regression-undersample-cm\", name=f\"{model_name.split('/')[-1]}_lr_{args.lr}_bs_{args.batch_size}_undersampled\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to init W&B: {e}. Continuing without W&B.\")\n",
        "        args.report_to = None\n",
        "\n",
        "    try:\n",
        "        train_dataset, test_dataset, tokenizer, num_labels = load_and_prepare_data(args, df)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Data preparation failed for {model_name}: {e}\")\n",
        "        raise\n",
        "\n",
        "    if args.do_train or not os.path.exists(os.path.join(args.model_path, args.model_name.split(\"/\")[-1])):\n",
        "        logger.info(f\"No trained model found at {args.model_path}/{args.model_name.split('/')[-1]}. Training new model.\")\n",
        "        trainer = train_model(args, train_dataset, test_dataset, num_labels)\n",
        "    else:\n",
        "        try:\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                os.path.join(args.model_path, args.model_name.split(\"/\")[-1]), num_labels=num_labels\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model for prediction {args.model_name}: {e}\")\n",
        "            raise\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=os.path.join(args.model_path, args.model_name.split(\"/\")[-1]),\n",
        "            per_device_eval_batch_size=args.batch_size,\n",
        "            fp16=True\n",
        "        )\n",
        "        trainer = CustomTrainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            compute_metrics=None\n",
        "        )\n",
        "\n",
        "    if args.do_predict:\n",
        "        predict(trainer, test_dataset, args)\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = pd.read_csv('chatbot_responses.csv', sep=',', on_bad_lines='skip', engine='python')\n",
        "        logger.info(f\"Raw CSV loaded with {len(df)} rows\")\n",
        "        data = df[['response', 'label', 'condition']]\n",
        "        if data.empty:\n",
        "            raise ValueError(\"Dataset is empty. Check input data.\")\n",
        "        logger.info(f\"Filtered dataset with {len(data)} rows\")\n",
        "        main(data, model_name=\"bert-base-uncased\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kxXy5zAs9IV",
        "outputId": "b21b8b4a-477c-4ccc-fae1-c0a177065bd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting regression2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python regression2.py --do_train --do_predict --num_train_epochs 7 --lr 1e-5 --batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5APVMArtGrM",
        "outputId": "ad854b92-8d7a-4c99-d145-9ca24c01deec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-28 09:10:24.327468: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-06-28 09:10:24.346305: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751101824.370053   43705 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751101824.377129   43705 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-28 09:10:24.399672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshachkia\u001b[0m (\u001b[33mshachkia-hebrew-university-of-jerusalem\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250628_091029-daekk6cm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_lr_1e-05_bs_16_undersampled\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/shachkia-hebrew-university-of-jerusalem/empathy-regression-undersample-cm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/shachkia-hebrew-university-of-jerusalem/empathy-regression-undersample-cm/runs/daekk6cm\u001b[0m\n",
            "Map: 100% 372/372 [00:00<00:00, 2804.72 examples/s]\n",
            "Map: 100% 499/499 [00:00<00:00, 3187.52 examples/s]\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "{'loss': 31.8258, 'grad_norm': 186.38026428222656, 'learning_rate': 9.702380952380953e-06, 'epoch': 0.42}\n",
            "{'loss': 21.6118, 'grad_norm': 117.65343475341797, 'learning_rate': 9.107142857142858e-06, 'epoch': 0.83}\n",
            "{'loss': 17.682, 'grad_norm': 131.83885192871094, 'learning_rate': 8.571428571428571e-06, 'epoch': 1.25}\n",
            "{'loss': 12.136, 'grad_norm': 97.15260314941406, 'learning_rate': 7.976190476190477e-06, 'epoch': 1.67}\n",
            "{'loss': 9.703, 'grad_norm': 95.21784973144531, 'learning_rate': 7.380952380952382e-06, 'epoch': 2.08}\n",
            "{'loss': 7.5398, 'grad_norm': 47.6678352355957, 'learning_rate': 6.785714285714287e-06, 'epoch': 2.5}\n",
            "{'loss': 6.8818, 'grad_norm': 20.69110870361328, 'learning_rate': 6.1904761904761914e-06, 'epoch': 2.92}\n",
            "{'loss': 5.2712, 'grad_norm': 50.71934127807617, 'learning_rate': 5.595238095238096e-06, 'epoch': 3.33}\n",
            "{'loss': 4.5184, 'grad_norm': 102.7938461303711, 'learning_rate': 5e-06, 'epoch': 3.75}\n",
            "{'loss': 4.864, 'grad_norm': 28.44139289855957, 'learning_rate': 4.404761904761905e-06, 'epoch': 4.17}\n",
            "{'loss': 3.8982, 'grad_norm': 33.238372802734375, 'learning_rate': 3.80952380952381e-06, 'epoch': 4.58}\n",
            "{'loss': 4.8724, 'grad_norm': 54.05635070800781, 'learning_rate': 3.2142857142857147e-06, 'epoch': 5.0}\n",
            "{'loss': 4.2677, 'grad_norm': 34.97618865966797, 'learning_rate': 2.6190476190476192e-06, 'epoch': 5.42}\n",
            "{'loss': 3.9558, 'grad_norm': 32.77256774902344, 'learning_rate': 2.023809523809524e-06, 'epoch': 5.83}\n",
            "{'loss': 4.3119, 'grad_norm': 30.5903263092041, 'learning_rate': 1.4285714285714286e-06, 'epoch': 6.25}\n",
            "{'loss': 4.0972, 'grad_norm': 38.10848617553711, 'learning_rate': 8.333333333333333e-07, 'epoch': 6.67}\n",
            "{'train_runtime': 29.0324, 'train_samples_per_second': 89.693, 'train_steps_per_second': 5.787, 'train_loss': 8.958211864743914, 'epoch': 7.0}\n",
            "100% 168/168 [00:29<00:00,  5.79it/s]\n",
            "100% 32/32 [00:00<00:00, 51.85it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mae ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_0 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_2 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_3 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_4 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_5 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_6 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_7 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_8 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             mae_score_9 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     mse ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_0 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_1 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_2 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_3 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_4 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_5 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_6 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_7 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_8 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sample_count_score_9 ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               test/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test/runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: test/samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test/steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██████\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/grad_norm █▅▆▄▄▂▁▂▄▁▂▂▂▂▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss █▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mae 3.03617\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_0 5.06055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_1 4.07031\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_2 3.02441\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_3 1.9968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_4 1.02734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_5 0.20521\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_6 1.0006\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_7 1.98466\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_8 2.97639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              mae_score_9 3.98239\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      mse 10.54784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_0 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_1 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_2 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_3 11\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_4 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_5 15\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_6 39\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_7 81\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_8 89\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sample_count_score_9 246\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                test/loss 10.54784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test/runtime 0.6419\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  test/samples_per_second 777.383\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/steps_per_second 49.852\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               total_flos 685135036551168.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/epoch 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        train/global_step 168\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/grad_norm 38.10849\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/loss 4.0972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_loss 8.95821\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_runtime 29.0324\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_samples_per_second 89.693\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_steps_per_second 5.787\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mbert-base-uncased_lr_1e-05_bs_16_undersampled\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/shachkia-hebrew-university-of-jerusalem/empathy-regression-undersample-cm/runs/daekk6cm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/shachkia-hebrew-university-of-jerusalem/empathy-regression-undersample-cm\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250628_091029-daekk6cm/logs\u001b[0m\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}